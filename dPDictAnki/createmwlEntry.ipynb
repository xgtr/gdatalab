{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ------ Load These Five Functions First ---\n",
    "\n",
    "# 1. tuneBymwBen(wlist=[], path='', wfile='')\n",
    "# 2. tuneReport(wlist=[], path='', wfile='')\n",
    "# 3. txt2wl(path='', wfile='')\n",
    "# 4. hw4learn(playtype=1, wdict={}, wlist=[], path='', wfile='', excluf='E:\\\\1Now\\\\taglist\\\\2939exclu.txt', note='')\n",
    "# 5. compareWordlist(l1=[], l2=[], path='', f1='', f2='')\n",
    "# 6. wordinAnki(ftype)    '''ftype: 1-soundPlay, 2-sPlay'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* has words :  106\n",
      "* has unique word number:  106\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  \n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  'Spotlight'\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Award-nominee\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Boston\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Boston's\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Brian\n",
      "inflection:  Directed --- hw:  direct\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  James\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Keaton\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Liev\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  McAdams\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  McCarthy\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Michael\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Prize-winning\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Pulitzer\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Rachel\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Ruffalo\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Schreiber\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Stanley\n",
      "inflection:  Starring --- hw:  star\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  Tucci\n",
      "inflection:  allegations --- hw:  allegation\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  biggest\n",
      "inflection:  cover-ups --- hw:  cover-up\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  d'Arcy\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  decades-long\n",
      "inflection:  delves --- hw:  delve\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  dramatic-thriller\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  highest\n",
      "inflection:  institutions --- hw:  institution\n",
      "---derivative:  investigation --- hw:  investigate\n",
      "inflection:  levels --- hw:  level\n",
      "---derivative:  molestation --- hw:  molest\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  newspaper's\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  oldest\n",
      "inflection:  reporters --- hw:  reporter\n",
      "inflection:  revelations --- hw:  revelation\n",
      "inflection:  shaking --- hw:  shake\n",
      "inflection:  steps --- hw:  step\n",
      "inflection:  tells --- hw:  tell\n",
      "inflection:  trusted --- hw:  trust\n",
      "inflection:  uncovered --- hw:  uncover\n",
      "inflection:  uncovers --- hw:  uncover\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  world's\n",
      "**************^^^^^^^!!@@@@@*************** is not in mwBen:  year-long\n",
      "* same as hw:  50\n",
      "* capitalized hw:  11\n",
      "* words are derivatives:  2\n",
      "* words are inflections:  15\n",
      "\n",
      "**************##########***************** All not in mwBen:  28 \n",
      "[u'', u\"'Spotlight'\", u'Award-nominee', u'Boston', u\"Boston's\", u'Brian', u'James', u'Keaton', u'Liev', u'McAdams', u'McCarthy', u'Michael', u'Prize-winning', u'Pulitzer', u'Rachel', u'Ruffalo', u'Schreiber', u'Stanley', u'Tucci', u'biggest', u\"d'Arcy\", u'decades-long', u'dramatic-thriller', u'highest', u\"newspaper's\", u'oldest', u\"world's\", u'year-long']\n",
      "\n",
      "---words to learn---:  20\n",
      "deck:sPlay ( hw:\"Catholic\" or hw:\"academy\" or hw:\"allegation\" or hw:\"archdiocese\" or hw:\"cover-up\" or hw:\"delve\" or hw:\"establishment\" or hw:\"globe\" or hw:\"investigative\" or hw:\"massive\" or hw:\"molest\" or hw:\"reporter\" or hw:\"revelation\" or hw:\"riveting\" or hw:\"scandal\" or hw:\"spotlight\" or hw:\"suspense\" or hw:\"tenacious\" or hw:\"tom\" or hw:\"uncover\")\n",
      "\n",
      "number of notes in Anki :  45\n",
      "number of unique words :  18\n",
      "---common words of list-1 and list-2---: 18\n",
      "---words only in list-1---: 0\n",
      "\n",
      "---words only in list-2---: 2\n",
      "globe, revelation\n"
     ]
    }
   ],
   "source": [
    "# --- You have an article, and want to extract a word list to learn in Anki (sentence or sound play) ---\n",
    "\n",
    "wlraw = txt2wl('E:\\\\1Now\\\\mae\\\\article\\\\', 'spotlight.txt')\n",
    "rnt = tuneReport(wlist=wlraw)\n",
    "# soundPlay = hw4learn(1, wdict=rnt)\n",
    "sPlay = hw4learn(2, wdict=rnt, note='Spotlight (2015) summary')\n",
    "\n",
    "compareWordlist(l1=wordinAnki(2), l2=sPlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------ E:\\1Now\\taglist\\46sounds2.txt ------\n",
      "* has words :  212\n",
      "* has unique word number:  212\n",
      "---derivative:  oceanic --- hw:  ocean\n",
      "* same as hw:  211\n",
      "* words are derivatives:  1\n",
      "\n",
      "---words to learn---:  212\n",
      "deck:soundPlay ( hw:\"McCoy\" or hw:\"Satan\" or hw:\"above\" or hw:\"abut\" or hw:\"acquisition\" or hw:\"adjective\" or hw:\"air\" or hw:\"aqua\" or hw:\"arduous\" or hw:\"arrive\" or hw:\"autumn\" or hw:\"azure\" or hw:\"baby\" or hw:\"banner\" or hw:\"bare\" or hw:\"battle\" or hw:\"bear\" or hw:\"beauty\" or hw:\"beer\" or hw:\"bet\" or hw:\"betel\" or hw:\"bird\" or hw:\"blue\" or hw:\"boar\" or hw:\"bone\" or hw:\"boor\" or hw:\"bosun\" or hw:\"boulder\" or hw:\"boy\" or hw:\"bread\" or hw:\"budget\" or hw:\"buzz\" or hw:\"captain\" or hw:\"cello\" or hw:\"chasm\" or hw:\"chauffeur\" or hw:\"cheetah\" or hw:\"choir\" or hw:\"circus\" or hw:\"coat\" or hw:\"coin\" or hw:\"collegiate\" or hw:\"comb\" or hw:\"conscious\" or hw:\"cornucopia\" or hw:\"cot\" or hw:\"cotton\" or hw:\"could\" or hw:\"cracked\" or hw:\"crazed\" or hw:\"crew\" or hw:\"cushion\" or hw:\"cute\" or hw:\"day\" or hw:\"days\" or hw:\"did\" or hw:\"do\" or hw:\"doe\" or hw:\"dollar\" or hw:\"door\" or hw:\"dummy\" or hw:\"dye\" or hw:\"earth\" or hw:\"easy\" or hw:\"eat\" or hw:\"egg\" or hw:\"emir\" or hw:\"err\" or hw:\"exaggerate\" or hw:\"example\" or hw:\"fact\" or hw:\"fade\" or hw:\"famous\" or hw:\"fan\" or hw:\"farther\" or hw:\"fascinate\" or hw:\"fascism\" or hw:\"father\" or hw:\"fern\" or hw:\"feud\" or hw:\"few\" or hw:\"fine\" or hw:\"flu\" or hw:\"fur\" or hw:\"gem\" or hw:\"genuflect\" or hw:\"ghost\" or hw:\"glazier\" or hw:\"go\" or hw:\"graduation\" or hw:\"grief\" or hw:\"guide\" or hw:\"hat\" or hw:\"hear\" or hw:\"hillock\" or hw:\"humdrum\" or hw:\"hurry\" or hw:\"idol\" or hw:\"ink\" or hw:\"journal\" or hw:\"joy\" or hw:\"key\" or hw:\"knot\" or hw:\"know\" or hw:\"laugh\" or hw:\"liar\" or hw:\"lie\" or hw:\"loud\" or hw:\"low\" or hw:\"luncheon\" or hw:\"lure\" or hw:\"machine\" or hw:\"main\" or hw:\"marinate\" or hw:\"mass\" or hw:\"mat\" or hw:\"match\" or hw:\"mattress\" or hw:\"me\" or hw:\"measure\" or hw:\"mission\" or hw:\"mnemonic\" or hw:\"myth\" or hw:\"nation\" or hw:\"nature\" or hw:\"nauseous\" or hw:\"no\" or hw:\"now\" or hw:\"ocean\" or hw:\"oceanic\" or hw:\"odd\" or hw:\"offer\" or hw:\"ogre\" or hw:\"oh\" or hw:\"one\" or hw:\"opinion\" or hw:\"patois\" or hw:\"pedal\" or hw:\"persuade\" or hw:\"physician\" or hw:\"pick\" or hw:\"pier\" or hw:\"port\" or hw:\"pour\" or hw:\"pretty\" or hw:\"prey\" or hw:\"pull\" or hw:\"question\" or hw:\"race\" or hw:\"receive\" or hw:\"red\" or hw:\"region\" or hw:\"rheumatism\" or hw:\"rhyme\" or hw:\"rich\" or hw:\"rough\" or hw:\"rubber\" or hw:\"satin\" or hw:\"sausage\" or hw:\"savvy\" or hw:\"saw\" or hw:\"say\" or hw:\"schism\" or hw:\"schist\" or hw:\"school\" or hw:\"see\" or hw:\"serious\" or hw:\"shah\" or hw:\"shy\" or hw:\"sign\" or hw:\"silent\" or hw:\"sing\" or hw:\"ski\" or hw:\"sly\" or hw:\"soccer\" or hw:\"soldier\" or hw:\"sorry\" or hw:\"special\" or hw:\"squirrel\" or hw:\"steak\" or hw:\"stop\" or hw:\"strenuous\" or hw:\"sudden\" or hw:\"sugar\" or hw:\"supper\" or hw:\"take\" or hw:\"tax\" or hw:\"telephone\" or hw:\"their\" or hw:\"there\" or hw:\"thin\" or hw:\"this\" or hw:\"tip\" or hw:\"tissue\" or hw:\"tour\" or hw:\"unit\" or hw:\"vein\" or hw:\"very\" or hw:\"vinyl\" or hw:\"vision\" or hw:\"war\" or hw:\"way\" or hw:\"weird\" or hw:\"whale\" or hw:\"woman\" or hw:\"wood\" or hw:\"world\" or hw:\"write\" or hw:\"xylophone\" or hw:\"yard\" or hw:\"youth\" or hw:\"zone\")\n"
     ]
    }
   ],
   "source": [
    "# --- You have a word list, and want to learn them in Anki (sentence or sound play) ---\n",
    "\n",
    "rnt = tuneReport(path='E:\\\\1Now\\\\taglist\\\\', wfile='46sounds2.txt')\n",
    "soundPlay = hw4learn(1, wdict=rnt, excluf='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- load 6 functions ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def tuneBymwBen(wlist=[], path='', wfile=''):        \n",
    "    \n",
    "    if wfile:\n",
    "        print '\\n\\n------', path+wfile, '------'\n",
    "        with codecs.open(path+wfile, 'r', 'utf-8') as f:\n",
    "            txtraw = f.read().splitlines()\n",
    "        txtraw = [x.strip() for x in txtraw]\n",
    "    elif wlist:\n",
    "        txtraw = [x.strip() for x in wlist]\n",
    "    else:\n",
    "        print 'function argumenents error'\n",
    "        return -1\n",
    "    \n",
    "    print '* has words : ', len(txtraw)\n",
    "    print '* has unique word number: ', len(set(txtraw))\n",
    "    dupinraw = set([x for x in txtraw if txtraw.count(x) > 1])\n",
    "    if dupinraw: print '* has duplicates: ', dupinraw\n",
    "   \n",
    "        \n",
    "    with codecs.open(\"E:\\\\1Now\\\\taglist\\\\mwlBenchmark.txt\", 'r', 'utf-8') as f:    \n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    # generate mwBen dictionary    \n",
    "    hws = [] # sorted unique hws\n",
    "    hwsC = [] # sorted unique capitalized hws\n",
    "#     mwBens = []\n",
    "    mwBenDerivs = []\n",
    "    mwBenInfls = []\n",
    "    \n",
    "    \n",
    "    for line in lines:\n",
    "#         if len(line) < 1:\n",
    "#             print 'len(line)<1', line \n",
    "#             return -1\n",
    "        hw = line.split(';')[2]\n",
    "        wtype = line.split(';')[0]\n",
    "                \n",
    "        entry = {}\n",
    "        if wtype == '2':            \n",
    "            entry['hw'] = hw\n",
    "            entry['deriv'] = line.split(';')[1] # derivative\n",
    "            mwBenDerivs.append(entry)\n",
    "        elif wtype == '3':            \n",
    "            entry['hw'] = hw\n",
    "            entry['infl'] = line.split(';')[1] # inflection\n",
    "            mwBenInfls.append(entry)\n",
    "                \n",
    "        hws.append(hw)\n",
    "        if hw[0] in string.lowercase[0:26]:\n",
    "            hwsC.append(hw.capitalize())\n",
    "        \n",
    "    hws = sorted(set(hws))\n",
    "    hwsC = sorted(set(hwsC))        \n",
    "\n",
    "\n",
    "    # begin to check the word list against mw benchmark dictionary\n",
    "    #  Word; Its-hw (Head Word)\n",
    "    #\n",
    "    # Three cases\n",
    "    # 1. word is exactly same as hw or is a capitalized hw (like 'Walk') or word.capitalize() is in hw (like 'leo')\n",
    "    # 2. word is a derivative or is a capitalized derivative (like 'Walks') or word.capitalize() is in derivate (like 'leos')\n",
    "    # 3. word is not in mwBen\n",
    "\n",
    "    \n",
    "    hwreturn = [] # hws returned by this function\n",
    "    \n",
    "    wordhw = [] # word is exactly same as hw\n",
    "    wordhwL = [] # word.lower() is a hw, like 'Walk'\n",
    "    wordhwC = [] # word.capitalize() is a hw, like 'leo'\n",
    "    \n",
    "    wordDeriv = [] # raw derivatives or capitalized derivatives or derivative.lower()\n",
    "    fineDeriv = [] # fine derivateivs are in mwBen\n",
    "    hwofDeriv = [] # hws of derivatives\n",
    "    \n",
    "    wordInfl = [] # raw inflections or capitalized inflections or inflection.lower()\n",
    "    fineInfl = [] # fine inflections are in mwBen\n",
    "    hwofInfl = [] # hws of inflections\n",
    "    \n",
    "    wordnotBen = [] # word is not in mwBen\n",
    "\n",
    "    for word in txtraw:\n",
    "        hwflag = 0 # suppose that word is not a hw\n",
    "        derivflag = 0 # suppose that word is not a derivative\n",
    "        inflflag = 0 # suppose that word is not a inflection\n",
    "        \n",
    "        # first check if word is a hw\n",
    "        if word in hws:\n",
    "            wordhw.append(word)\n",
    "            hwreturn.append(word)\n",
    "            hwflag = 1\n",
    "        elif word.capitalize() in hws: # like leo\n",
    "            wordhwC.append(word)\n",
    "            hwreturn.append(word.capitalize())\n",
    "            hwflag = 1            \n",
    "        elif word.capitalize() in hwsC: # like Walk, tAke\n",
    "            wordhwL.append(word)\n",
    "            hwreturn.append(word.lower())\n",
    "            hwflag = 1\n",
    "\n",
    "        # if word isn't a hw, then check if it's a derivative\n",
    "        if hwflag == 0:\n",
    "            dercnt = 0\n",
    "            for mwben in mwBenDerivs:                \n",
    "                if word == mwben['deriv'] or word == mwben['deriv'].capitalize() or word.capitalize() == mwben['deriv']:\n",
    "                    wordDeriv.append(word)\n",
    "                    fineDeriv.append(mwben['deriv'])\n",
    "                    hwofDeriv.append(mwben['hw'])\n",
    "                    derivflag = 1\n",
    "                    print '---derivative: ', word, '--- hw: ', mwben['hw']\n",
    "                    dercnt += 1\n",
    "            if dercnt > 1: print word, '--- derivate has multiple hws --- ', dercnt\n",
    "\n",
    "        if hwflag == 0 and derivflag == 0:\n",
    "            inflcnt = 0\n",
    "            for mwben in mwBenInfls:                \n",
    "                if word == mwben['infl'] or word == mwben['infl'].capitalize() or word.capitalize() == mwben['infl']:\n",
    "                    wordInfl.append(word)\n",
    "                    fineInfl.append(mwben['infl'])\n",
    "                    hwofInfl.append(mwben['hw'])\n",
    "                    inflflag = 1\n",
    "                    print 'inflection: ', word, '--- hw: ', mwben['hw']\n",
    "                    inflcnt += 1\n",
    "            if inflcnt > 1: print word, '--- inflection has multiple hws --- ', inflcnt                \n",
    "                \n",
    "                \n",
    "        if hwflag == 0 and derivflag == 0 and inflflag == 0:\n",
    "            if word.find(' ') == -1:\n",
    "                wordnotBen.append(word)\n",
    "                print '**************^^^^^^^!!@@@@@*************** is not in mwBen: ', word\n",
    "            else:\n",
    "                print '>>> >>> >>> >>> >> not a single word: ', word\n",
    "                for x in word.split(' '):\n",
    "                    if x in hws: hwreturn.append(x)\n",
    "                    elif x.capitalize() in hws: hwreturn.append(x.capitalize())\n",
    "                    elif x in hwsC: hwreturn.append(x.lower())\n",
    "                    else:\n",
    "                        wdflagx = 0 \n",
    "                        for mwben in mwBenDerivs:                \n",
    "                            if x == mwben['deriv'] or x == mwben['deriv'].capitalize() \\\n",
    "                            or x.capitalize() == mwben['deriv']:\n",
    "                                wordDeriv.append(word)\n",
    "                                fineDeriv.append(mwben['deriv'])\n",
    "                                hwofDeriv.append(mwben['hw'])\n",
    "                                wdflagx = 1\n",
    "                        if wdflagx == 0:\n",
    "                            for mwben in mwBenInfls:                \n",
    "                                if x == mwben['infl'] or x == mwben['infl'].capitalize() \\\n",
    "                                or x.capitalize() == mwben['infl']:\n",
    "                                    wordInfl.append(word)\n",
    "                                    fineInfl.append(mwben['infl'])\n",
    "                                    hwofInfl.append(mwben['hw'])\n",
    "                                    wdflagx = 1\n",
    "                        if wdflagx == 0:\n",
    "                            wordnotBen.append(x)\n",
    "                            print '**************^^^^^^^!!@@@@@*************** really is not in mwBen: ', x\n",
    "                                \n",
    "                            \n",
    "            \n",
    "    \n",
    "    if wordhw: print '* same as hw: ', len(wordhw)  #,' -- ', wordhw\n",
    "    if wordhwL: print '* capitalized hw: ', len(wordhwL)  #,' -- ', wordhwL\n",
    "    if wordhwC: print '* hw typo, should be capitalized: ', len(wordhwC)  #,' -- ', wordhwC\n",
    "    \n",
    "    if wordDeriv: print '* words are derivatives: ', len(wordDeriv) #,' -- ', wordDeriv\n",
    "    if wordInfl: print '* words are inflections: ', len(wordInfl) #,' -- ', wordInfl\n",
    "\n",
    "    if wordnotBen:\n",
    "        print '\\n**************##########***************** All not in mwBen: ', len(wordnotBen), '\\n', wordnotBen\n",
    "        \n",
    "#     if len(txtraw) == len(wordhw) + len(wordhwL) + len(wordhwC) + len(wordDeriv) + len(wordnotBen):         \n",
    "#     else: print \"--- words' numbers don't mattch, check ---\\n\"\n",
    "\n",
    "        \n",
    "    rntdic = {}    \n",
    "    rntdic['tunehw'] = sorted(set(hwreturn))\n",
    "    rntdic['hwofDeriv'] = sorted(set(hwofDeriv))\n",
    "    rntdic['tuneDeriv'] = sorted(set(fineDeriv))\n",
    "    rntdic['hwofInfl'] = sorted(set(hwofInfl))\n",
    "    rntdic['tuneInfl'] = sorted(set(fineInfl))\n",
    "    rntdic['rawhwderiv'] = sorted(set(wordhw + wordhwL + wordhwC + wordDeriv + wordInfl))\n",
    "    rntdic['notinben'] = sorted(set(wordnotBen))\n",
    "    \n",
    "    return rntdic\n",
    "    \n",
    "# tuneBymwBen(path='E:\\\\1Now\\\\taglist\\\\', wfile='testmwBen.txt') \n",
    "\n",
    "\n",
    "def tuneReport(wlist=[], path='', wfile=''):\n",
    "    if wlist: fname = 'tuneReport-list.txt'\n",
    "    elif wfile: fname = 'tuneReport-' + wfile\n",
    "    else:\n",
    "        print 'arguments error'\n",
    "        return -1\n",
    "    \n",
    "    rnt = tuneBymwBen(wlist, path, wfile)\n",
    "    \n",
    "    tunehw = rnt['tunehw']\n",
    "    hwofDeriv = rnt['hwofDeriv']\n",
    "    tuneDeriv = rnt['tuneDeriv'] \n",
    "    hwofInfl = rnt['hwofInfl']\n",
    "    tuneInfl = rnt['tuneInfl']\n",
    "    notinben = rnt['notinben']    \n",
    "    #rawhwderiv = rnt['rawhwderiv']\n",
    "    \n",
    "    fd = codecs.open('E:\\\\1Now\\\\mae\\\\report\\\\' + fname, 'w', 'utf-8')\n",
    "    if notinben:\n",
    "        fd.write('---not in mwBen---:' + str(len(notinben)) + '\\n')\n",
    "        for x in notinben: fd.write(x + '\\n')\n",
    "    if tunehw:\n",
    "        fd.write('\\n\\n--- tuned hws of raw hws which are in mwBen---: ' + str(len(tunehw)) + '\\n')\n",
    "        for x in tunehw: fd.write(x + '\\n')\n",
    "    if hwofDeriv:\n",
    "        fd.write('\\n\\n--- tuned hws of raw derivatives which are in mwBen---: ' + str(len(hwofDeriv)) + '\\n')\n",
    "        for x in hwofDeriv: fd.write(x + '\\n')\n",
    "    if tuneDeriv:\n",
    "        fd.write('\\n\\n--- all tuned derivatives which are in mwBen---: ' + str(len(tuneDeriv)) + '\\n')\n",
    "        for x in tuneDeriv: fd.write(x + '\\n') \n",
    "    if hwofInfl:\n",
    "        fd.write('\\n\\n--- tuned hws of raw inflections which are in mwBen---: ' + str(len(hwofInfl)) + '\\n')\n",
    "        for x in hwofInfl: fd.write(x + '\\n')\n",
    "    if tuneInfl:\n",
    "        fd.write('\\n\\n--- all tuned inflections which are in mwBen---: ' + str(len(tuneInfl)) + '\\n')\n",
    "        for x in tuneInfl: fd.write(x + '\\n')             \n",
    "\n",
    "    fd.close()\n",
    "    return rnt\n",
    "       \n",
    "# tuneReport(path='E:\\\\1Now\\\\taglist\\\\', wfile='testmwBen.txt')\n",
    "\n",
    "\n",
    "\n",
    "def txt2wl(path='', wfile=''):\n",
    "    '''extract a word list from a text, return a sorted unqiue raw word list'''\n",
    "    if wfile:\n",
    "        with codecs.open(path + wfile, 'r', 'utf-8') as f:\n",
    "            parags = f.read().splitlines() # each paragraph as one line\n",
    "    else:\n",
    "        print 'argument error'\n",
    "        return -1\n",
    "    \n",
    "    wlraw = [] # raw word list from the text\n",
    "    for parag in parags:\n",
    "        regxp = re.compile(r'[^\\w\\'\\-]+', re.U) # regular expression pattern: ^\\w\\'\\- means not words ' -\n",
    "        parag = regxp.sub(' ', parag) # replace all not words ' - with a space\n",
    "        wlraw += parag.split(' ')\n",
    "    \n",
    "    return sorted(set(wlraw))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hw4learn(playtype=1, wdict={}, wlist=[], path='', wfile='', excluf='E:\\\\1Now\\\\taglist\\\\2939exclu.txt', note=''):\n",
    "    '''This function only excludes hws and their derivatives exactly in mwl.\n",
    "    So it's better to use tuneBymwBen() first to get a fine word list\n",
    "    playtype=1 (sound play)\n",
    "    playtype=2 (sentence play)\n",
    "    '''\n",
    "  \n",
    "    if wdict:\n",
    "        if playtype == 1:  words = wdict['tunehw'] + wdict['hwofInfl'] + wdict['tuneDeriv']\n",
    "        elif playtype ==2: words = wdict['tunehw'] + wdict['hwofDeriv'] + wdict['hwofInfl']        \n",
    "    elif wlist:\n",
    "        words = [x.strip() for x in wlist]\n",
    "    elif wfile:\n",
    "        with codecs.open(path + wfile, 'r', 'utf-8') as f:\n",
    "            words = f.read().splitlines()\n",
    "        words = [x.strip() for x in words]\n",
    "    else:\n",
    "        print 'argument error'\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    word4learn = []       \n",
    "    if excluf:\n",
    "        with codecs.open(excluf, 'r', 'utf-8') as f:\n",
    "            bexclus = f.read().splitlines()    \n",
    "        for word in words:\n",
    "            if word not in bexclus: word4learn.append(word)\n",
    "    else:\n",
    "        word4learn = words\n",
    "            \n",
    "    word4learn = sorted(set(word4learn))\n",
    "    \n",
    "    # generate Anki search string for filtered deck    \n",
    "    if playtype == 1:   ankiSearchStr = 'deck:soundPlay ('\n",
    "    elif playtype == 2: ankiSearchStr = 'deck:sPlay ('\n",
    "        \n",
    "    for x in word4learn:\n",
    "        if x != word4learn[-1]: ankiSearchStr += ' hw:\"' + x + '\" or'\n",
    "        else: ankiSearchStr += ' hw:\"' + x + '\")'\n",
    "    print '\\n---words to learn---: ', len(word4learn)\n",
    "    print ankiSearchStr\n",
    "    \n",
    "    if note:\n",
    "        fw = codecs.open('E:\\\\1Now\\\\mae\\\\wordDaily.txt', 'a', 'utf-8')\n",
    "        fw.write('\\n#' + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '\\n' + note + '\\n')\n",
    "        fw.write('words: ' + str(len(word4learn)) + '\\n' + ankiSearchStr + '\\n')\n",
    "        fw.close()\n",
    "        \n",
    "    return word4learn\n",
    "\n",
    "\n",
    "def compareWordlist(l1=[], l2=[], path='', f1='', f2=''):\n",
    "    '''get common and different parts of two lists or word lists of two files'''\n",
    "    \n",
    "    if l1:\n",
    "        words1 = l1\n",
    "#         words1 = [x.strip() for x in l1]\n",
    "        f1name = 'list-1'\n",
    "    if l2:\n",
    "        words2 = l2\n",
    "#         words2 = [x.strip() for x in l2]\n",
    "        f2name = 'list-2'\n",
    "    if f1:\n",
    "        with codecs.open(path+f1, 'r', 'utf-8') as f:\n",
    "            words1 = f.read().splitlines()\n",
    "#         words1 = [x.strip() for x in words1]\n",
    "        f1name = f1\n",
    "    if f2:\n",
    "        with codecs.open(path+f2, 'r', 'utf-8') as f:\n",
    "            words2 = f.read().splitlines()\n",
    "#         words2 = [x.strip() for x in words2]\n",
    "        f2name = f2\n",
    "            \n",
    "    comm = [x for x in words1 if x in words2]\n",
    "    only1 = [x for x in words1 if x not in words2]\n",
    "    only2 = [x for x in words2 if x not in words1]\n",
    "    \n",
    "    comm = sorted(set(comm))\n",
    "    \n",
    "    f = codecs.open('E:\\\\1Now\\\\mae\\\\report\\\\compare_' + f1name + '_' + f2name + '.txt', 'w', 'utf-8')\n",
    "    \n",
    "    print '---common words of ' + f1name + ' and ' + f2name + '---: ' + str(len(comm))\n",
    "    f.write('---common words of ' + f1name + ' and ' + f2name + '---: ' + str(len(comm)) + '\\n')\n",
    "    for x in comm:\n",
    "        f.write(x + '\\n')\n",
    "    \n",
    "    print '---words only in ' + f1name + '---: ' + str(len(only1)) + '\\n' + ', '.join(only1)\n",
    "    f.write('\\n\\n---words only in ' + f1name + '---: ' + str(len(only1)) + '\\n')\n",
    "    for x in only1:\n",
    "        f.write(x + '\\n')    \n",
    "    \n",
    "    print '---words only in ' + f2name + '---: ' + str(len(only2)) + '\\n' + ', '.join(only2)\n",
    "    f.write('\\n\\n---words only in ' + f2name + '---: ' + str(len(only2)) + '\\n')\n",
    "    for x in only2:\n",
    "        f.write(x + '\\n')\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    \n",
    "# check number of words and notes in a filtered deck\n",
    "# first export the filted deck in Anki\n",
    "\n",
    "def wordinAnki(ftype):\n",
    "    '''ftype: 1-soundPlay, 2-sPlay'''\n",
    "    \n",
    "    if ftype == 1:\n",
    "        fname = 'E:\\\\1Now\\\\mae\\\\article\\\\filter_soundPlay.txt'\n",
    "        idx = 0\n",
    "    elif ftype == 2: \n",
    "        fname = 'E:\\\\1Now\\\\mae\\\\article\\\\filter_sPlay.txt'\n",
    "        idx = 4\n",
    "        \n",
    "    with codecs.open(fname, 'r', 'utf-8') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    print '\\nnumber of notes in Anki : ', len(set(lines))\n",
    "    words = [x.split('\\t')[idx] for x in lines]\n",
    "    words = sorted(words)\n",
    "    print 'number of unique words : ', len(set(words))\n",
    "    \n",
    "    return words\n",
    "\n",
    "#     b = {}\n",
    "#     for x in words:     \n",
    "#         b[x] = words.count(x)\n",
    "#     #     print x, b[x]\n",
    "#     print len(b)\n",
    "\n",
    "#     cnt = 0\n",
    "\n",
    "#     for x in b:\n",
    "#         #print x, b[x]\n",
    "#         cnt += b[x]\n",
    "#     print 'number of notes in Anki: ', cnt    \n",
    "    \n",
    "print '--- load 6 functions ---'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of excluded hw:  2939\n",
      "total head words:  5011\n",
      "total words:  31600\n",
      "unique words: 9833\n"
     ]
    }
   ],
   "source": [
    "# Organize 'beautifulExcluHw.txt' and generate 'beautifulExclu.txt'\n",
    "# (note: words in beautifulExcluHw.txt are absolutely hws in 39195 of mwl)\n",
    "# directly from mwaled mongodb dpdict. words not being a hw in mwaled won't be added to the two lists\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# codecs if for read and write files with unicode, encoding 'utf-8'\n",
    "import codecs\n",
    "\n",
    "#connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.dpdb  # databas: dpdb\n",
    "geweiDict = db.geweiDict201511 # collection (table): geweiDict201511\n",
    "#geweiDict = db.geweiDictTest\n",
    "\n",
    "\n",
    "with codecs.open(\"E:\\\\1Now\\\\taglist\\\\2939bw.txt\", 'r', 'utf-8') as f:\n",
    "    behws = f.read().splitlines()\n",
    "    f.close()    \n",
    "# sort and make unique words in beautifulExcluHw.txt\n",
    "behws = sorted(set(behws))\n",
    "print 'number of excluded hw: ', len(behws)\n",
    "\n",
    "# fd = codecs.open(\"E:\\\\1Now\\\\mae\\\\beautifulExcluHw.txt\", 'w', 'utf-8')\n",
    "# for behw in behws:\n",
    "#     fd.write(behw + '\\n')\n",
    "# fd.close()\n",
    "    \n",
    "fdict = codecs.open('E:\\\\1Now\\\\mae\\\\beautifulExcluraw.txt', 'w', 'utf-8')\n",
    "    \n",
    "results = geweiDict.find({}, {\"_id\":0, \"hw\":1, \"infl\":1, \"subhw\":1}) #.limit(1000)\n",
    "\n",
    "# total number of head words\n",
    "totalhw = 0\n",
    "totalcnt = 0\n",
    "\n",
    "for result in results:    \n",
    "    word = result.get(\"hw\")    \n",
    "\n",
    "    if word and word in behws:\n",
    "        #print word\n",
    "#         fdict.write(word + ';' + word + '\\n')\n",
    "        fdict.write(word + '\\n')\n",
    "        totalhw += 1\n",
    "    else:\n",
    "#         print \"no head word\"\n",
    "        continue\n",
    "\n",
    "    # get items of result\n",
    "    infls = result.get(\"infl\")\n",
    "    subhws = result.get(\"subhw\")     \n",
    "    \n",
    "    if infls:\n",
    "        for infl in infls:\n",
    "#             fdict.write(infl + ';' + word + '\\n')\n",
    "            fdict.write(infl + '\\n')\n",
    "            totalcnt += 1   \n",
    "            \n",
    "    if subhws:\n",
    "        for subhw in subhws:\n",
    "            if subhw[0] != word[0]: continue # check if subhw is a real sub hw of word by comparing first letter\n",
    "#             fdict.write(subhw + ';' + word  + '\\n')\n",
    "            fdict.write(subhw + '\\n')\n",
    "            totalcnt += 1        \n",
    "    \n",
    "fdict.close()\n",
    "print \"total head words: \", totalhw\n",
    "print \"total words: \", totalcnt + totalhw\n",
    "\n",
    "\n",
    "# sort out by head words and make them unique\n",
    "with codecs.open('E:\\\\1Now\\\\mae\\\\beautifulExcluraw.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "lines = sorted(set(lines))\n",
    "print 'unique words:', len(lines)\n",
    "#lines = sorted(lines)\n",
    "\n",
    "fd = codecs.open('E:\\\\1Now\\\\mae\\\\beautifulExclu.txt', 'w', 'utf-8')\n",
    "for line in lines:\n",
    "    fd.write(line + '\\n')\n",
    "fd.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mwBen statistics ---\n",
      "total head words:  39195\n",
      "total inflections:  64355\n",
      "total subhws:  8493\n",
      "total subhws whose 1st letters are different from hws:  158\n",
      "hw without any derivatives:  11221\n",
      "\n",
      "\n",
      "number of lines in mwBen before adding 84en_fr.txt:  84069\n",
      "number of unique lines in mwBen before adding 84en_fr.txt:  53853\n",
      "number of lines in 84en_fr.txt:  84\n",
      "number of unique lines in 84en_fr.txt:  83\n",
      "---common words of list-1 and list-2---: 15\n",
      "---words only in list-1---: 68\n",
      "---words only in list-2---: 53838\n",
      "number of lines in mwBen after adding 84en_fr.txt:  53921\n"
     ]
    }
   ],
   "source": [
    "# output mwlBenchmark.txt (mwBen) (no duplicate lines)\n",
    "# 2015-11-28, 2015-11-30, 2015-12-01, 2015-12-08\n",
    "\n",
    "# sort and make every line unique\n",
    "# format: n;hw, derivative, or inflection;hw\n",
    "# 1 - head word; 2 - derivative; 3 - inflection\n",
    "\n",
    "# 1;truly;truly\n",
    "# 2;blazingly;blazing\n",
    "# 3;blazes;blaze\n",
    "# 3;blazing;blaze\n",
    "\n",
    "# By the lines above, I can know blaze and blazing are hws, the two lines below are not allowd \n",
    "# because hws have derivates or inflections:\n",
    "# blaze;blaze\n",
    "# blazing;blazing\n",
    "# the two lines above are redundant.\n",
    "\n",
    "# manually delete: asas\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# codecs if for read and write files with unicode, encoding 'utf-8'\n",
    "import codecs\n",
    "\n",
    "#connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.dpdb  # databas: dpdb\n",
    "geweiDict = db.geweiDict201511 # collection (table): geweiDict201511\n",
    "#geweiDict = db.geweiDictTest\n",
    "\n",
    "fdict = codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmarkraw.txt', 'w', 'utf-8')\n",
    "    \n",
    "results = geweiDict.find({}, {\"_id\":0, \"hw\":1, \"infl\":1, \"subhw\":1}) #.limit(1000)\n",
    "# sort method will use much more memory\n",
    "#results = geweiDict.find({}, {\"_id\":0, \"hw\":1, \"prn\":1, \"alles\":1, \"idpvs\":1, \"sublist\":1}).sort(\"hw\", 1).limit(1000)\n",
    "\n",
    "\n",
    "hwcnt = 0 # number of head words (hws)\n",
    "# hwcapcnt = 0 # number of hws capitalized\n",
    "inflcnt = 0 # number of inflections\n",
    "subhwcnt = 0 # number of subhws\n",
    "subhw1cnt = 0 # number of subhws whose first letter is not same as hw\n",
    "onlyhwcnt = 0\n",
    "\n",
    "# subhw1hw = []\n",
    "\n",
    "for result in results:\n",
    "    inflflag = 0\n",
    "    subhwflag = 0\n",
    "    \n",
    "    word = result.get(\"hw\")    \n",
    "    if word:\n",
    "        #print word\n",
    "#         fdict.write(word + ';' + word + '\\n')\n",
    "        hwcnt += 1      \n",
    "    else:\n",
    "        print \"no head word\"\n",
    "        continue\n",
    "\n",
    "    # get items of result\n",
    "    subhws = result.get(\"subhw\")\n",
    "    infls = result.get(\"infl\")         \n",
    "    \n",
    "    if subhws:\n",
    "        for subhw in subhws:\n",
    "            #if subhw[0] != word[0]: continue       \n",
    "            if subhw.lower() == word.lower():\n",
    "                #print 'subhw == word', subhw\n",
    "                continue           \n",
    "            if infls:\n",
    "                if subhw in infls:\n",
    "                    #print '----subhw in infls:', subhw\n",
    "                    continue\n",
    "                    \n",
    "            if subhw[0] != word[0]:\n",
    "                #print 'subhw is not like word: ', subhw, '...', word\n",
    "                subhw1cnt += 1\n",
    "                #subhw1hw.append(subhw+';'+word)\n",
    "            fdict.write('2;' + subhw + ';' + word + '\\n')\n",
    "            subhwcnt += 1      \n",
    "            subhwflag = 1    \n",
    "    \n",
    "    \n",
    "    if infls:\n",
    "        for infl in infls:\n",
    "            if infl.lower() == word.lower():\n",
    "                #print 'infl == word', infl\n",
    "                continue\n",
    "            fdict.write('3;' + infl + ';' + word + '\\n')\n",
    "            inflcnt += 1   \n",
    "            inflflag = 1\n",
    "            \n",
    "            \n",
    "    # hw;hw (only when hw has no inflections and no subhws)\n",
    "    if inflflag == 0 and subhwflag == 0:\n",
    "        fdict.write('1;' + word + ';' + word + '\\n')\n",
    "        onlyhwcnt += 1\n",
    "    \n",
    "fdict.close()\n",
    "\n",
    "print '--- mwBen statistics ---'\n",
    "print \"total head words: \", hwcnt\n",
    "# print 'total hws capitalized: ', hwcapcnt\n",
    "print \"total inflections: \", inflcnt\n",
    "print 'total subhws: ', subhwcnt\n",
    "print 'total subhws whose 1st letters are different from hws: ', subhw1cnt\n",
    "print 'hw without any derivatives: ', onlyhwcnt\n",
    "\n",
    "\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmarkraw.txt', 'r', 'utf-8') as f:\n",
    "    mwls = f.read().splitlines()\n",
    "print '\\n\\nnumber of lines in mwBen before adding 84en_fr.txt: ', len(mwls)\n",
    "mwls = sorted(set(mwls))\n",
    "print 'number of unique lines in mwBen before adding 84en_fr.txt: ', len(mwls)\n",
    "\n",
    "# read en_fr.txt\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\84en_fr.txt', 'r', 'utf-8') as f:\n",
    "    enfrs = f.read().splitlines()\n",
    "print 'number of lines in 84en_fr.txt: ', len(enfrs)\n",
    "enfrs = sorted(set(enfrs))\n",
    "print 'number of unique lines in 84en_fr.txt: ', len(set(enfrs))\n",
    "\n",
    "compareWordlist(l1=enfrs, l2=mwls)\n",
    "\n",
    "# sort and make unique\n",
    "lines = sorted(set(mwls + enfrs))\n",
    "print 'number of lines in mwBen after adding 84en_fr.txt: ', len(lines)\n",
    "\n",
    "fd = codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmark.txt', 'w', 'utf-8')\n",
    "for line in lines:\n",
    "    fd.write(line + '\\n')\n",
    "fd.close()\n",
    "\n",
    "\n",
    "# subhw1hw = sorted(set(subhw1hw))\n",
    "# fd = codecs.open('E:\\\\1Now\\\\taglist\\\\subhw1hw.txt', 'w', 'utf-8')\n",
    "# for x in subhw1hw:\n",
    "#     fd.write(x + '\\n')\n",
    "# fd.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- explore mwBen ---\n",
      "number of derivative column:  53972\n",
      "number of hw column:  53972\n",
      "unique number of hw column:  34398\n",
      "number of derivatives (inflections and subhws) whose 1st letter different from hws:  337\n",
      "number of capitalized hws:  1487\n"
     ]
    }
   ],
   "source": [
    "# explore mwBen\n",
    "\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "with codecs.open('E:\\\\1Now\\\\taglist\\\\mwlBenchmark.txt', 'r', 'utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "deriv = [] # derivative\n",
    "benhw = [] # hw\n",
    "deriv1hw = [] # derivative (inflection, subhw) first letter different from its hw\n",
    "hwcap = [] # hw whose first letter is capitalized\n",
    "for line in lines:\n",
    "    x1 = line.split(';')[0]\n",
    "    x2 = line.split(';')[1]\n",
    "    deriv.append(x1)\n",
    "    benhw.append(x2)\n",
    "    if x1[0] != x2[0]: deriv1hw.append(line)\n",
    "    if x2[0] in string.uppercase[0:26]: hwcap.append(x2)\n",
    "derivS = sorted(deriv)\n",
    "benhwSU = sorted(set(benhw)) # Sorted, Unique\n",
    "hwcap = sorted(set(hwcap))\n",
    "\n",
    "print '--- explore mwBen ---'\n",
    "print 'number of derivative column: ', len(deriv)\n",
    "print 'number of hw column: ', len(benhw)\n",
    "print 'unique number of hw column: ', len(benhwSU)\n",
    "print 'number of derivatives (inflections and subhws) whose 1st letter different from hws: ', len(deriv1hw)\n",
    "print 'number of capitalized hws: ', len(hwcap)\n",
    "\n",
    "fout = codecs.open('E:\\\\1Now\\\\taglist\\\\mwlStat.txt', 'w', 'utf-8')\n",
    "fout.write('--- statistics ---\\n')\n",
    "fout.write('number of lines in mwBen: ' + str(len(deriv)) + '\\n')\n",
    "fout.write('unique number of hw: ' + str(len(benhwSU)) + '\\n')\n",
    "fout.write('number of derivatives (inflections and subhws) whose 1st letter different from hws: ' + str(len(deriv1hw)) + '\\n')\n",
    "fout.write('number of capitalized hws: ' + str(len(hwcap)) + '\\n')\n",
    "\n",
    "fout.write('Contents\\n--------\\n')\n",
    "fout.write(\"Q: Are there duplicate derivatives who aren't hws in mwBen?\\n\")\n",
    "fout.write(\"Q: List derivatives (inflections and subhws) whose first letter is different from the first letter of their hws\\n\")\n",
    "fout.write(\"Capitalized hws\\n\\n\")\n",
    "\n",
    "# Q: Are there duplicate derivatives who aren't hws in mwBen?\n",
    "dupderiv = [x for x in derivS if x not in benhwSU and derivS.count(x)>1]\n",
    "fout.write(\"--- Duplicate derivatives not being hws: ---\\n\")\n",
    "for x in dupderiv:\n",
    "    fout.write(x + '\\n')\n",
    "\n",
    "# Q: List subhws whose first letter is different from the first letter of their hws\n",
    "fout.write(\"\\n--- derivatives (inflections and subhws) whose 1st letter different from hws: ---\\n\")\n",
    "fout.write(\"derivatives (inflections and subhws);hw\\n\\n\")\n",
    "for x in subhw1hw:\n",
    "    fout.write(x + '\\n')\n",
    "\n",
    "fout.write(\"\\n--- Capitalized hws: ---\\n\")\n",
    "for x in hwcap:\n",
    "    fout.write(x + '\\n')\n",
    "           \n",
    "fout.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
